# -*- coding: utf-8 -*-
"""Copy of  Fake or True news using Neural Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHqAiezX0UMmpwDuPHz9VvPzNceOqIyx
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn import tree
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
from wordcloud import WordCloud
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

"""Reading Fake ***news***"""

nltk.download('stopwords')

#Loading the data
from google.colab import drive

drive.mount('/content/drive')

fake = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fake.csv')
true = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/True.csv')

data= fake+true

data.head(5)

data = data.sample(frac=1).reset_index(drop=True)

df= data.copy()
df.drop( ['date'], inplace= True, axis=1)

df.head(5)

print(df.info())
print("---------------")
print(data.head())
print("---------------")
print(data.describe())

data.isnull().sum()

df.dtypes

#Counting by Subjects
for key,count in fake.subject.value_counts().iteritems():
    print(f"{key}:\t{count}")

#Getting Total Rows
print(f"Total Records:\t{fake.shape[0]}")
#Word Cloud
text = ''
for news in fake.text.values:
    text += f" {news}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(text)
fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del text

true = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/True.csv')

true.head()

#Counting by Subjects
for key,count in true.subject.value_counts().iteritems():
    print(f"{key}:\t{count}")

#Getting Total Rows
print(f"Total Records:\t{true.shape[0]}")
#Word Cloud
text = ''
for news1 in true.text.values:
    text += f" {news1}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(text)
fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del text

import re

def is_twitter_text(text):
    # Regular expressions for Twitter-related patterns
    twitter_patterns = [
        r"@\w+",         # Mention (@username)
        r"#\w+",         # Hashtag (#hashtag)
        r"https?://\S+", # URLs
    ]

    # Check if any of the patterns match in the text
    for pattern in twitter_patterns:
        if re.search(pattern, text):
            return True

    return False


is_twitter_text('title')

"""**Cleaning Data**

Removing Reuters or Twitter Tweet information from the text

Text can be splitted only once at " - " which is always present after mentioning source of publication, this gives us publication part and text part
If we do not get text part, this means publication details was't given for that record
The Twitter tweets always have same source, a long text of max 259 characters
add Codeadd Markdown
"""

real = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/True.csv')
real.head()

#First Creating list of index that do not have publication part
unknown_publishers = []
for index,row in enumerate(real.text.values):
    try:
        record = row.split(" -", maxsplit=1)
        #if no text part is present, following will give error
        record[1]
        #if len of piblication part is greater than 260
        #following will give error, ensuring no text having "-" in between is counted
        assert(len(record[0]) < 260)
    except:
        unknown_publishers.append(index)

# we have list of indices where publisher is not mentioned
#lets check
real.iloc[unknown_publishers].text
#true, they do not have text like "WASHINGTON (Reuters)"

real.iloc[8970]

#Seperating Publication info, from actual text
publisher = []
tmp_text = []
for index,row in enumerate(real.text.values):
    if index in unknown_publishers:
        #Add unknown of publisher not mentioned
        tmp_text.append(row)

        publisher.append("Unknown")
        continue
    record = row.split(" -", maxsplit=1)
    publisher.append(record[0])
    tmp_text.append(record[1])

#Replace existing text column with new text
#add seperate column for publication info
real["publisher"] = publisher
real["text"] = tmp_text

del publisher, tmp_text, record, unknown_publishers

real.head()

#checking for rows with empty text like row:8970
[index for index,text in enumerate(real.text.values) if str(text).strip() == '']

#dropping this record
real = real.drop(8970, axis=0)

# checking for the same in fake news
empty_fake_index = [index for index,text in enumerate(fake.text.values) if str(text).strip() == '']
print(f"No of empty rows: {len(empty_fake_index)}")
fake.iloc[empty_fake_index].tail()

# checking for the same in fake news
empty_fake_index = [index for index,text in enumerate(fake.text.values) if str(text).strip() == '']
print(f"No of empty rows: {len(empty_fake_index)}")
fake.iloc[empty_fake_index].tail()

sns.countplot(x="subject", data=real)
plt.show()

#WordCloud For Real News
text = ''
for news in real.text.values:
    text += f" {news}"
wordcloud = WordCloud(
    width = 3000,
    height = 2000,
    background_color = 'black',
    stopwords = set(nltk.corpus.stopwords.words("english"))).generate(str(text))
fig = plt.figure(
    figsize = (40, 30),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
del text

fake = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fake.csv')
real = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/True.csv')

"""***Preproccesing Data***"""

fake['class']= 0
real['class']= 1

#Combining Title and Text
real["text"] = real["title"] + " " + real["text"]
fake["text"] = fake["title"] + " " + fake["text"]

# Subject is diffrent for real and fake thus dropping it
# Aldo dropping Date, title and Publication Info of real
real = real.drop(["subject", "date","title"], axis=1)
fake = fake.drop(["subject", "date", "title"], axis=1)

#Combining both into new dataframe
data = real.append(fake, ignore_index=True)
del real, fake

nltk.download('stopwords')
nltk.download('punkt')

"""Removing StopWords, Punctuations and single-character words

"""

y = data["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X.append(tmp)

del data

"""**### Vectorization -- Word2Vec**

Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.

Word embedding is the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.


"""

import gensim

from gensim.test.utils import common_texts
import gensim.models
from gensim.models import Word2Vec

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by Word2Vec Method (takes time...)
w2v_model = Word2Vec(sentences=common_texts, vector_size=EMBEDDING_DIM, window=5, min_count=1)

# Dimension of vectors we are generating
EMBEDDING_DIM = 100
# Creating Word Vectors by Word2Vec Method (takes time...)
w2v_model = gensim.models.Word2Vec(sentences=X, vector_size=EMBEDDING_DIM, window=5, min_count=1)

#vocab size
len(w2v_model.wv.key_to_index)

#We have now represented each of 122248 words by a 100dim vector.

"""**Exploring Vectors**

Lets checkout these vectors
"""

#see a sample vector for random word, lets say Corona

w2v_model.wv["corona"]
w2v_model.wv.most_similar("iran")
w2v_model.wv.most_similar("fbi")
w2v_model.wv.most_similar("facebook")
w2v_model.wv.most_similar("computer")

#Feeding US Presidents
w2v_model.wv.most_similar(positive=["trump","obama", "clinton"])
#First was Bush

"""***Tokenizing Text***
These Vectors will be passed to LSTM/GRU instead of words. 1D-CNN can further be used to extract features from the vectors.


Keras has implementation called "**Embedding Layer**" which would create word embeddings(vectors). Since we did that with gensim's word2vec, we will load these vectors into embedding layer and make the layer non-trainable.


"""

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer

#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

# lets check the first 10 words of first news
#every word has been represented with a number
X[0][:10]

#Lets check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 10:
        break

"""We can pass numerical representation of words into neural network.

We can use Many-To-One (Sequence-To-Word) Model of RNN, as we have many words in news as input and one output ie Probability of being Real.

For Many-To-One model, lets use a fixed size input.

"""

# For determining size of input...

# Making histogram for no of words in news shows that most news article are under 700 words.
# Lets keep each news small and truncate all news to 700 while tokenizing
plt.hist([len(x) for x in X], bins=500)
plt.show()

# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :)

nos = np.array([len(x) for x in X])
len(nos[nos  < 700])
# Out of 48k news, 44k have less than 700 words

#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones
maxlen = 700

#Making all news of size maxlen defined above
X = pad_sequences(X, maxlen=maxlen)

#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0
# 0 is not associated to any word, as mapping of words started from 1
# 0 will also be used later, if unknows word is encountered in test set
len(X[0])

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus our vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

"""We Create a matrix of mapping between word-index and vectors. We use this as weights in embedding layer

Embedding layer accepts numecical-token of word and outputs corresponding vercor to inner layer.

It sends vector of zeros to next layer for unknown words which would be tokenized to 0.


Input length of Embedding Layer is the length of each news (700 now due to padding and truncating)
"""

#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer
embedding_vectors = get_weight_matrix(w2v_model.wv, word_index)

#Defining Neural Network
model = Sequential()
#Non-trainable embeddidng layer
model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))
#LSTM
model.add(LSTM(units=128))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

del embedding_vectors

model.summary()

#Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y)

model.fit(X_train, y_train, validation_split=0.3, epochs=6)

y_pred = (model.predict(X_test) > 0.5).astype("int")

accuracy_score(y_test, y_pred)

from google.colab import drive
drive.mount('/content/drive')

print(classification_report(y_test, y_pred))

from flask import Flask, request, jsonify
import pickle

app = Flask(__name__)

# Load the SVM model
with open('svm_model.pkl', 'rb') as model_file:
    svm_model = pickle.load(model_file)

# Define API endpoint for predictions
@app.route('/predict', methods=['POST'])
def predict():
    # Get input data from the application
    data = request.get_json()

    # Process the input data if required
    # Perform predictions using the SVM model
    prediction = svm_model.predict([data['input']])  # Example: 'input' is the key for input data

    # Return the prediction
    return jsonify({'prediction': str(prediction[0])})

if __name__ == '__main__':
app.run(debug=True)

import security

sec =
encoded_string = security.encode(string, ‘ascii’)
print(encoded_string)
decoded_string = security.decode(encoded_string, ‘ascii’)
print(decoded_string)